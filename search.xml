<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>由浅入深了解优化器(Optimizer)</title>
    <url>/2023/04/08/%E4%BC%98%E5%8C%96%E5%99%A8/</url>
    <content><![CDATA[<h1 id="由浅入深了解优化器-Optimizer"><a href="#由浅入深了解优化器-Optimizer" class="headerlink" title="由浅入深了解优化器(Optimizer)"></a>由浅入深了解优化器(Optimizer)</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><blockquote>
<p>1.什么是优化器？</p>
</blockquote>
<blockquote>
<p>2.有哪些优化器</p>
</blockquote>
<blockquote>
<p>3.优化算法的选择与使用策略</p>
</blockquote>
<h2 id="1-什么是优化器？"><a href="#1-什么是优化器？" class="headerlink" title="1.什么是优化器？"></a>1.什么是优化器？</h2><h3 id="（1）解释"><a href="#（1）解释" class="headerlink" title="（1）解释"></a>（1）解释</h3><p>  优化器就是在深度学习反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数值不断逼近全局最小。</p>
<span id="more"></span>
<p><img src="F:\Blog\source\images\D.png" alt="图片"></p>
<h3 id="（2）原理解释"><a href="#（2）原理解释" class="headerlink" title="（2）原理解释"></a>（2）原理解释</h3><p>优化问题可以看作是我们站在山上的某个位置（当前参数的位置），想要以最佳的路线去到山下（最优点）。首先，直观的方法是环顾四周，找到下山的最快的方向走一步，然后再次环顾四周，找到最快的方向，直到下山————这样的方法就是最普通的梯度下降————当前的海拔是我们的目标函数值，而我们的每一步找到的方向就是函数梯度的反方向（梯度是函数上升最快的方向，所以梯度的反方向就是函数下降最快的方向）</p>
<p>事实上，使用梯度下降进行优化，是所有优化器的核心思想。当我们下山时，有两个方面是我们最关心的:</p>
<blockquote>
<p>首先是优化方向，决定“前进的方向是否正确”，在优化器中反映为梯度或动量。</p>
</blockquote>
<blockquote>
<p>其次是步长，决定“每一步迈多远”，在优化器中反映为学习率。</p>
</blockquote>
<p>所有优化器都在关注这两个方面，但同时也有一些其他问题，比如应该从哪里出发，路线错误应该如何处理……这是一些最新优化器在关注的方向。</p>
<h3 id="（3）公式和定义"><a href="#（3）公式和定义" class="headerlink" title="（3）公式和定义"></a>（3）公式和定义</h3><p>待优化参数： ω，目标函数： 𝑓(𝑥)，初始学习率: α ，迭代的epoch：t</p>
<p>参数更新步骤如下：</p>
<p>1.计算目标函数关于当前参数的梯度：</p>
<blockquote>
<p>g(𝑡)&#x3D;∇𝑓(𝑤𝑡)</p>
</blockquote>
<p>2.根据历史梯度计算一阶动量和二阶动量：</p>
<blockquote>
<p>$𝑚_𝑡&#x3D;𝜙(𝑔_1,…,𝑔_𝑡 )$</p>
</blockquote>
<p>3.计算当前时刻的下降梯度：</p>
<blockquote>
<p>𝜂_𝑡&#x3D;𝑎*𝑚_𝑡∕√(𝑣_𝑡 )</p>
</blockquote>
<p>4.根据下降梯度进行更新参数：</p>
<blockquote>
<p>𝜔_(𝑡+1)&#x3D;𝑤<em>𝑡−𝜂</em>(𝑡−1)</p>
</blockquote>
<p>步骤3，4对于各个算法基本上都是一致的，主要差别就体现在步骤1，2上。</p>
<p>##2.有哪些优化器？</p>
<h3 id="2-1随机梯度下降法-Stochastic-Gradient-Descent-SGD"><a href="#2-1随机梯度下降法-Stochastic-Gradient-Descent-SGD" class="headerlink" title="2.1随机梯度下降法(Stochastic Gradient Descent,SGD)"></a>2.1随机梯度下降法(Stochastic Gradient Descent,SGD)</h3><p>随机梯度下降法每次从训练集中随机选择一个样本来进行学习，SGD没有动量的概念，因此：</p>
<p>$$<br>m_t &#x3D; g_t; V_t &#x3D; I^2<br>$$</p>
<p>代入步骤3，可以得到下降梯度：<br>$$<br>\eta_t &#x3D; \alpha \times g_t<br>$$</p>
<p><strong>SGD参数更新公式</strong>如下，其中α是学习率，g_t是当前参数梯度<br>$$<br>w_{t+1} &#x3D; w_t - \eta_t &#x3D; w_t - \alpha \times g_t<br>$$</p>
<p>优点：</p>
<p>（1）每次只用一个样本更新模型参数，训练速度快<br>（2）随机梯度下降所带来的波动有利于优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。</p>
<p>缺点：</p>
<p>（1）当遇到局部最优点或者鞍点的时候，梯度为0，无法继续更新参数。这时候它天真的以为找到了最优解，其实不然。<br><img src="F:\Blog\source\images\E.png"></p>
<p>（2）沿着陡峭方向振荡，而沿着平缓维度进展缓慢，难以迅速收敛。</p>
<h3 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h3><p>为了抑制SGD的振荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候如果发现是陡坡，那就利用惯性跑的快一点，SGDM全称是SGD with momentum，它的重点就在于后面的momentum，因为它在SGD的基础上增加了一阶动量的选项。</p>
<p>$$<br>m_t &#x3D; \beta_1 \times m_{t-1} + (1-\beta_1)\times g_t<br>$$</p>
<p><strong>SGD-M参数更新公式</strong>如下，其中α是学习率，g_t是当前参数的梯度：<br>$$<br>w_{t+1} &#x3D; w_t - \alpha \times m_t &#x3D; w_t - \alpha \times (\beta_1 \times m_{t-1} + (1-\beta_1)\times g_t)<br>$$</p>
<p>一阶动量是各个时刻梯度方向的指数移动平均值，也就是说，t时刻下降的方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。β_1的经验值为0.9，这就意味着下降方向主要是此前积累的下降方向。其实我们举个现实的例子就能知道，引用此前的动量这个想法是一个很棒的方法。比如我们在高速公路上漂移转弯，我们一般都会踩刹车来漂移，主要依靠汽车此前运动的累积动量运动。因为急转弯可是要出大事的（藤原拓海开的AE86除外）</p>
<p>特点：</p>
<p>因为加入了动量的因素，SGD-M缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题，但是并没有完全解决，当局部沟壑比较深的时候，动量加持用完了，依然会困在局部最优里来回振荡。<br><img src="F:\Blog\source\images\F.png"></p>
<h3 id="2-3-SGD-with-Nesterov-Acceleration"><a href="#2-3-SGD-with-Nesterov-Acceleration" class="headerlink" title="2.3 SGD with Nesterov Acceleration"></a>2.3 SGD with Nesterov Acceleration</h3><p>SGD还有一个问题是困在局部最优的沟壑里振荡。想象你走到一个盆地里，四周都是略高的小山，你觉得没有下坡的方向，那就只能呆在这里了。可是如果你爬上高低，就会发现外面的世界很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步。</p>
<p>NAG全称为Nesterov Acceleration Gradient，是在SGD、SGD-M上的改进，改进点在于步骤1，我们知道在时刻t的主要方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前的梯度方向，不如先看看如果跟着累积动量走了一步，到时候看一步走一步。因此，NAG在步骤1上，不计当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：<br>$$<br>g_t &#x3D; \nabla f(w_t - \alpha \centerdot m_{t-1}&#x2F;\sqrt{V_{t-1}})<br>$$</p>
<p><strong>NAG参数更新公式</strong>如下，其中α是学习率，g_t是当前参数的梯度<br>$$<br>w_{t+1}&#x3D;w_t - \alpha \centerdot g_t &#x3D; w_t - \alpha \times (\nabla f(w_t - \alpha \centerdot m_{t-1}&#x2F;\sqrt{V_{t-1}}))<br>$$</p>
<p>然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量。</p>
<p>特点：有利于跳出当前局部最优沟壑，寻找新的最优值，但是收敛速度慢。</p>
<h3 id="2-4-AdaGrad（自适应学习率算法）"><a href="#2-4-AdaGrad（自适应学习率算法）" class="headerlink" title="2.4 AdaGrad（自适应学习率算法）"></a>2.4 AdaGrad（自适应学习率算法）</h3><p>SGD系列的都没有用到二阶动量。二阶动量的出现，才意味着往“自适应学习率”优化算法的到来。SGD及其变种以同样的学习率更新每个参数，但是对于深度神经网络往往包含大量的参数，这些参数并不是总会用得到的。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率能慢一点；对于偶尔更新的参数，我们了解的信息太少了，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</p>
<p>怎么用去度量历史更新频率？</p>
<p>那就是二阶动量————该维度上，记录到目前为止所有梯度值的平方和：<br>$$<br>V_t &#x3D; \sum_{\tau &#x3D; 1}^t {g_{\tau}^2}<br>$$</p>
<p>我们再回顾以下步骤三的下降梯度：<br>$$<br>\eta &#x3D; \alpha \centerdot m_t&#x2F;\sqrt{V_t}<br>$$</p>
<p><strong>AdaGrad参数更新公式</strong>如下，其中α是学习率，g_t是当前参数的梯度：<br>$$<br>w_{t+1} &#x3D; w_t - \frac{\alpha \centerdot m_t}{\sqrt{\sum_{\tau &#x3D; 1}^t g_{\tau}^2}}<br>$$</p>
<p>可以看出，此时实质上的学习率由α变成了α&#x2F;sqrt(V_t).一般为了避免分母为0，会在分母上加一个小的平滑项。因此sqrt(V_t)是恒大于0的，而且参数更新的越频繁，二阶动量越大，学习率越小。</p>
<p>优点：</p>
<p>（1）在稀疏数据场景下表现得非常好</p>
<p> (2) 此前的SGD及其变体的优化器主要聚焦在优化梯度前进的方向上，而AdaGrad首次使用二阶动量来关注学习率（步长），开启了自适应学习率算法的里程。</p>
<p>缺点：</p>
<p> 因为sqrt(V_t)是单调递增的，会使得学习率单调递减至0，可能使得训练过程提前结束，即便后续还有数据也无法学习到必要的知识。</p>
<h3 id="2-5-AdaDelta-x2F-RMSProp"><a href="#2-5-AdaDelta-x2F-RMSProp" class="headerlink" title="2.5 AdaDelta&#x2F;RMSProp"></a>2.5 AdaDelta&#x2F;RMSProp</h3><p>由于AdaGrad单调递减的学习率变化过于激进，考虑一个改变二阶动量的计算方法的策略：不累计全部历史梯度，而只关注过去一段时间窗口的下降速度。这就是AdaDe名称Delta的来历。</p>
<p>修改的思路很简单，前面讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：</p>
<p>$$<br>V_t &#x3D; \beta_2 \centerdot V_{t-1} + (1-\beta_2)\centerdot g_t^2<br>$$</p>
<p><strong>AdaDelta&#x2F;RMSProp参数更新公式</strong></p>
<p>$$<br>w_{t+1} &#x3D; w_t - \frac{\alpha \centerdot m_t}{\sqrt{\beta_2 \centerdot V_{t-1}+(1-\beta_2)g_t^2}}<br>$$<br>优点：</p>
<p>避免了二阶动量持续累积，导致训练过程提前结束的问题。</p>
<h3 id="2-6-Adam"><a href="#2-6-Adam" class="headerlink" title="2.6 Adam"></a>2.6 Adam</h3><p>SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam————Adaptive + Momentum</p>
<p>SGD的一阶动量：</p>
<p>$$<br>m_t &#x3D; \beta_1 \centerdot m_{t-1} + (1-\beta_1)\centerdot g_t<br>$$</p>
<p>加上AdaDelta的二阶动量：</p>
<p>$$<br>V_t &#x3D; \beta_2 \centerdot V_{t-1} + (1-\beta_2)g_t^2<br>$$</p>
<p><strong>Adam参数更新公式</strong>如下：<br>$$<br>w_{t+1} &#x3D; w_t - \frac{\alpha \centerdot m_t}{\sqrt{V_t}}<br>$$</p>
<p>优化算法里最常见的两个超参数β1和β2就在这里，前者控制一阶动量，后者控制二阶动量。</p>
<p>优点：</p>
<p>（1）通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和在鞍点的静止。</p>
<p>缺点：</p>
<p><strong>（1）可能不收敛：</strong></p>
<p>二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得V_t可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。</p>
<p>修正的方法。由于Adam中的学习率主要是由二阶动量控制的，为了保证算法的收敛，可以对二阶动量的变化进行控制，避免上下波动。</p>
<p>$$<br>V_t &#x3D; max(\beta_2 \times V_{t-1}+(1-\beta_2)g_t^2,V_{t-1})<br>$$</p>
<p>通过这样修改，就保证了||V_t|| &gt;&#x3D; ||V_t-1||，从而使得学习率单调递减。</p>
<p>**（2）可能会错过全局最优解 **</p>
<p>自适应学习率算法可能对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。后期Adam学习率太低，影响了有效的收敛。</p>
<h3 id="2-7-Nadam"><a href="#2-7-Nadam" class="headerlink" title="2.7 Nadam"></a>2.7 Nadam</h3><p>它是在Adam的基础上，增加了带有Nesterov加速度梯度计算的思想</p>
<p>Nesterov + Adam &#x3D; Nadam</p>
<p>按照NAG的步骤：<br>$$<br>g_t &#x3D; \nabla f(w_t - \frat{\alpha m_{t-1}}{\sqrt{V_t}})<br>$$</p>
<h2 id="3-优化算法的选择与使用策略"><a href="#3-优化算法的选择与使用策略" class="headerlink" title="3. 优化算法的选择与使用策略"></a>3. 优化算法的选择与使用策略</h2><p>优化算法的常用tricks</p>
<p>（1）首先，各大算法孰优孰劣并无定论。如果是刚入门，优先考虑SGD+Nesterov Momentum或者Adam。</p>
<p>（2）选择你熟悉的算法——这样你可以更加熟练地利用你的经验进行调参。</p>
<p>（3）充分了解你的数据——如果模型是非常稀疏的，那么优先考虑自适应学习率的算法。</p>
<p>（4）根据你的需求来选择——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。</p>
<p>（5）先用小数据集进行实验。有论文研究指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。</p>
<p>（6）考虑不同算法的组合。先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。</p>
<p>（7）数据集一定要充分的打散（shuffle）。这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。</p>
<p>（8）训练过程中持续监控训练数据和验证数据上的目标函数值以及精度或者AUC等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。</p>
<p>（9）制定一个合适的学习率衰减策略。可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。</p>
]]></content>
      <categories>
        <category>note</category>
        <category>深度学习</category>
        <category>优化器</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkdownPad 使用教程</title>
    <url>/2023/03/30/MakedownPad%E4%BD%BF%E7%94%A8%E5%B8%AE%E5%8A%A9/</url>
    <content><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>标题</li>
<li>列表</li>
<li>引用</li>
<li>图片和超链接</li>
<li>粗体与斜体</li>
<li>代码框</li>
<li>转义字符</li>
<li>其他快捷键</li>
<li>最后</li>
</ol>
<h3 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h3><p>标题的设置很简单，有三种添加方式：①手动添加；②快捷键添加；③选项卡添加，下面分别对这三种方式进行介绍。</p>
<p>Ⅰ. 手动添加</p>
<p>一级标题对应的是#一级标题#，二级标题对应##二级标题##，以此类推。</p>
<p>Ⅱ. 快捷键添加</p>
<p>直接在键盘上输入ctrl+num就能得到对应标题的级数</p>
<p>Ⅲ. 选项卡设置</p>
<p>在菜单栏中选择插入选项卡，找到需要插入的<strong>标题</strong>,添加即可。</p>
<span id="more"></span>
<h3 id="2-列表"><a href="#2-列表" class="headerlink" title="2.列表"></a>2.列表</h3><p>2.1 有序列表<br>有序列表，就是对输入的内容按序号进行标识排列。有三种添加方式：</p>
<ol>
<li>手动添加</li>
<li>快捷键添加</li>
<li>选项卡添加</li>
</ol>
<p>①手动添加<br>在输入的内容前，添加num. + 空格 ，接着按 回车 ，即可添加 num+1 条内容。<br>注意：1）这里的 num 是指排列的序号，一般第一行内容，num-&gt;1 ，如果 num 设置成其他数值，则依然从 1 开始计数；2）num.的后边务必加空格 ，否则添加列表无效。</p>
<p>②快捷键添加<br>直接在键盘上输入Ctrl+Shift+O，输入第一条内容后，按 回车 ，即可添加第二条内容。注意快捷键输入的是 字母O ，而不是 数字0。</p>
<p>③选项卡添加<br>在菜单栏中选择 插入 选项卡，找到 有序列表 ，添加即可，或者在标题栏中，直接点击 添加有序列表的按钮。</p>
<p>2.2 无序列表<br>无序列表，顾名思义，即输入的内容不按序号进行排列。有三种添加方式：①手动添加；②快捷键添加；③选项卡添加，下面分别对这三种方式进行介绍。</p>
<p>①手动添加<br>在输入的内容前，添加-&#x2F;* + 空格 ，接着按 回车 ，即可添加下一条内容。</p>
<p>注意：1）这里的-&#x2F;* 是指-或<em>；2）-&#x2F;</em> 的后边务必加 空格 ，否则添加列表无效。</p>
<p>②快捷键添加<br>直接在键盘上输入Ctrl+U，输入第一条内容后，按 回车 ，即可添加第二条内容。</p>
<p>③选项卡添加<br>在菜单栏中选择 插入 选项卡，找到 无序列表 ，添加即可，或者在标题栏中，直接点击 添加的无序列表按钮。</p>
<h3 id="3-引用"><a href="#3-引用" class="headerlink" title="3.引用"></a>3.引用</h3><p>引用可以将一段文字进行单独的标识，撇如下面这个例子。</p>
<p>引用有三种添加方式：</p>
<p>Ⅰ. 手动添加：在需要标注引用的内容之前输入&gt;,可以将对应的内容标记为引用类型。</p>
<p>Ⅱ. 快捷键添加：直接在键盘上输入Ctrl+Q。</p>
<p>Ⅲ. 选项卡添加：在菜单栏中选择<strong>插入（insert）</strong>选项卡。</p>
<h3 id="4-图片和超链接"><a href="#4-图片和超链接" class="headerlink" title="4.图片和超链接"></a>4.图片和超链接</h3><p>4.1 图片</p>
<p>有时我们需要将网上的图片或者电脑上的图片，添加到md文件中进行展示，所以这时候我们就需要用到插入图片的语法。</p>
<p>在键盘中输入![图片名称]（链接名称）可以显示对应的图片。</p>
<p>但是如果输入的是</p>
<blockquote>
<p>[图片名称]（链接名称）</p>
</blockquote>
<p>就会显示的是对应的链接入口</p>
<h3 id="5-粗体和斜体"><a href="#5-粗体和斜体" class="headerlink" title="5.粗体和斜体"></a>5.粗体和斜体</h3><p>5.1 粗体</p>
<p>粗体的添加有三种方式:</p>
<ol>
<li>手动添加: 在需要加粗的文字或段落前加上**,然后语句后面也加上上述的符号.</li>
<li>快捷键添加:ctrl+B</li>
<li>选项卡添加: bold的选项可以插入粗体结构.</li>
</ol>
<p>5.2斜体</p>
<p>斜体的添加方式也是有三种:</p>
<p>添加方式与粗体类似</p>
<ol>
<li>手动添加: 不同于粗体的结构,斜体需要的是利用单个*号把想把改变的语句包围住.</li>
<li>快捷键添加: CTRL + I</li>
<li>选项卡设置: 找到 italic点击插入斜体结构.</li>
</ol>
<h3 id="6-代码"><a href="#6-代码" class="headerlink" title="6.代码"></a>6.代码</h3><p>6.1 代码行</p>
<p>输入一行代码,在代码前后添加&#96;,同时要注意是在英文输入法中添加,输入快捷键ctrl+K也可以对其进行添加.下面举例:</p>
<p><code>printf(&#39;hello world&#39;);</code></p>
<pre><code>hello world
</code></pre>
<p>6.2 代码块:</p>
<p>添加代码块网上给出了很多方法.有一种比较简单的方法.选中需要标记的代码段<br>利用以下语法：</p>
<p>--- bash</p>
<p>$ 代码内容</p>
<p>---</p>
<h3 id="7-转义字符"><a href="#7-转义字符" class="headerlink" title="7.转义字符"></a>7.转义字符</h3><p>这个和C语言的转义字符类似,在需要添加的特殊字符前添加\即可.</p>
<h3 id="8-其他快捷键"><a href="#8-其他快捷键" class="headerlink" title="8.其他快捷键"></a>8.其他快捷键</h3><p>8.1添加时间戳<br>键盘输入ctrl+T,即可从系统中获取时间,例如:3&#x2F;30&#x2F;2023 8:57:50 PM </p>
<p>8.2添加分割线<br>键盘输入ctrl+R,即可添加水平标尺.</p>
<hr>
]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>MarkdownPad</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/03/26/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a><br>comment: true</p>
]]></content>
  </entry>
</search>
